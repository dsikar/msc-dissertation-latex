\chapter{Network architectures} % Main appendix title

\label{AppendixB} % For referencing this appendix elsewhere, use \ref{AppendixX}

\section{NVIDIA baseline architecture}
\label{NVIDIA_baseline}
This is the starting point after the work of \cite{bojarski2016end}

\begin{verbatim}
>>> import models
>>> mymodel = models.nvidia_baseline(1)
>>> models.show_model_summary(mymodel)
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
img_in (InputLayer)          [(None, 66, 200, 3)]      0         
_________________________________________________________________
lambda (Lambda)              (None, 66, 200, 3)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 31, 98, 24)        1824      
_________________________________________________________________
dropout (Dropout)            (None, 31, 98, 24)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 14, 47, 36)        21636     
_________________________________________________________________
dropout_1 (Dropout)          (None, 14, 47, 36)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 5, 22, 48)         43248     
_________________________________________________________________
dropout_2 (Dropout)          (None, 5, 22, 48)         0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 3, 20, 64)         27712     
_________________________________________________________________
dropout_3 (Dropout)          (None, 3, 20, 64)         0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 1, 18, 64)         36928     
_________________________________________________________________
dropout_4 (Dropout)          (None, 1, 18, 64)         0         
_________________________________________________________________
flattened (Flatten)          (None, 1152)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 1164)              1342092   
_________________________________________________________________
dropout_5 (Dropout)          (None, 1164)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 100)               116500    
_________________________________________________________________
dropout_6 (Dropout)          (None, 100)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 50)                5050      
_________________________________________________________________
dropout_7 (Dropout)          (None, 50)                0         
_________________________________________________________________
dense_4 (Dense)              (None, 10)                510       
_________________________________________________________________
dropout_8 (Dropout)          (None, 10)                0         
_________________________________________________________________
steering (Dense)             (None, 1)                 11        
=================================================================
Total params: 1,595,511
Trainable params: 1,595,511
Non-trainable params: 0
_________________________________________________________________
[(None, 66, 200, 3)]
(None, 66, 200, 3)
(None, 31, 98, 24)
(None, 31, 98, 24)
(None, 14, 47, 36)
(None, 14, 47, 36)
(None, 5, 22, 48)
(None, 5, 22, 48)
(None, 3, 20, 64)
(None, 3, 20, 64)
(None, 1, 18, 64)
(None, 1, 18, 64)
(None, 1152)
(None, 1164)
(None, 1164)
(None, 100)
(None, 100)
(None, 50)
(None, 50)
(None, 10)
(None, 10)
(None, 1)
\end{verbatim}
\section{NVIDIA1 architecture}

The network architecture is as shown, with a total of 817,028 trainable parameters.
\begin{verbatim}
>>> import models
>>> mymodel = models.get_nvidia_model1(2)
>>> models.show_model_summary(mymodel)
Model: "model_3"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
img_in (InputLayer)          [(None, 120, 160, 3)]     0         
_________________________________________________________________
lambda_3 (Lambda)            (None, 120, 160, 3)       0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 58, 78, 24)        1824      
_________________________________________________________________
dropout_15 (Dropout)         (None, 58, 78, 24)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 27, 37, 32)        19232     
_________________________________________________________________
dropout_16 (Dropout)         (None, 27, 37, 32)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 12, 17, 64)        51264     
_________________________________________________________________
dropout_17 (Dropout)         (None, 12, 17, 64)        0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 10, 15, 64)        36928     
_________________________________________________________________
dropout_18 (Dropout)         (None, 10, 15, 64)        0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 8, 13, 64)         36928     
_________________________________________________________________
dropout_19 (Dropout)         (None, 8, 13, 64)         0         
_________________________________________________________________
flattened (Flatten)          (None, 6656)              0         
_________________________________________________________________
dense_6 (Dense)              (None, 100)               665700    
_________________________________________________________________
dense_7 (Dense)              (None, 50)                5050      
_________________________________________________________________
steering_throttle (Dense)    (None, 2)                 102       
=================================================================
Total params: 817,028
Trainable params: 817,028
Non-trainable params: 0
_________________________________________________________________
[(None, 120, 160, 3)]
(None, 120, 160, 3)
(None, 58, 78, 24)
(None, 58, 78, 24)
(None, 27, 37, 32)
(None, 27, 37, 32)
(None, 12, 17, 64)
(None, 12, 17, 64)
(None, 10, 15, 64)
(None, 10, 15, 64)
(None, 8, 13, 64)
(None, 8, 13, 64)
(None, 6656)
(None, 100)
(None, 50)
(None, 2)    
\end{verbatim}

\section{Training and testing networks}
A network is trained by running the script:
\begin{verbatim}
$ python --model=nvidia1
    --outdir=../trained_models
    --epochs=1
    --inputs='../dataset/unity/roboleague/log/*.jpg'
    --aug=false
    --crop=false    
\end{verbatim}
This will generate a model, saved in the .h5 format, a python dictionary object with training and testing accuracy and loss values, and an accuracy and loss plot with information additional labels to help identify the trained model. Finally, a log file is saved to disk containing model name, training time and last recorded loss and accuracy values for training and testing datasets.

run experiment on vanilla code (no augmentation)
Record cash crash and deviation from actual to predicted steering angles.

\section{Correspondence with authors}
\label{corr_with_authors}

\subsection{Correspondence with Urs Muller 1}
\label{urs_muller1}
Correspondence with \textbf{Urs Muller}, co-author of \cite{bojarski2016end}, with respect to network training settings.

\begin{verbatim}
From: Urs Muller <umuller@nvidia.com>Sent: 16 November 2020 19:30
To: PG-Sikar, Daniel <Daniel.Sikar@city.ac.uk>
Subject: Re: End to End Learning for Self-Driving Cars - Network Training Parameters

CAUTION: This email originated from outside of the organisation. 
Do not click links or open attachments unless you recognise the
sender and believe the content to be safe.

Hi Daniel,

We used the following settings (we haven't documented them in any publication):

loss function: MSE
optimizer: adadelta
learning rate: 1e-4 (but not really used in adadelta)
dropout: 0.25

Best regards,
Urs

From: PG-Sikar, Daniel <Daniel.Sikar@city.ac.uk>
Sent: Sunday, November 15, 2020 6:53To: Urs Muller <umuller@nvidia.com>
Subject: End to End Learning for Self-Driving Cars - Network Training Parameters
 
External email: Use caution opening links or attachments

Hi,

With respect to your network training, I am investigating the effect of noise 
(rainy images) on network performance, using your architecture as a baseline.

Would you be able to point me towards any documentation detailing loss function, 
learning rate, optimizer and layer droupout probabilities used when training 
your network?

Thanks in advance for any help and kind regards,

Daniel Sikar | MSc Data Science Candidate
School of Mathematics, Computer Science and Engineering
City University of London    
\end{verbatim}

\subsection{Correspondence with Urs Muller 2}
\label{urs_muller2}
\begin{verbatim}
Urs Muller <umuller@nvidia.com>
Tue 17/11/2020 11:47
CAUTION: This email originated from outside of the organisation. Do not click links or open attachments unless you recognise the sender and believe the content to be safe.

Hi Daniel,

Yes, correct. We cropped everything above the horizon. The lower edge of the crop is as low as possible before the road gets blocked by the hood of the car.

Best regards,
Urs
From: PG-Sikar, Daniel <Daniel.Sikar@city.ac.uk>
Sent: Tuesday, November 17, 2020 2:56
To: Urs Muller <umuller@nvidia.com>
Subject: Re: End to End Learning for Self-Driving Cars - Network Training Parameters
 
External email: Use caution opening links or attachments
Hi Urs,

Thanks for your reply. One more question with respect to the image size presented to your network - 66 pixel height by 200 width. Was this cropped from the size your camera sensors were generating? To keep only the road in the frame and omit anything above the horizon?

Kind regards,

Daniel    
\end{verbatim}

\subsection{Correspondence with Ankit Vora}
\label{ankit_vora}
Correspondence with \textbf{Ankit Vora}, co-author of \cite{agarwal2020ford}, with respect to extracting steering angles from rosbag files.

\begin{verbatim}
Vora, Ankit (A.) <avora3@ford.com>
Thu 05/11/2020 13:55
CAUTION: This email originated from outside of the organisation. 
Do not click links or open attachments unless you recognise the
sender and believe the content to be safe.

Hi Daniel,

The information you are looking for will be in the /pose_ground_truth message. 
That message represents the position and orientation of the vehicle.
You will have to convert quaternions to yaw, pitch, roll angles and
then use the yaw values.

Ankit
From: PG-Sikar, Daniel <Daniel.Sikar@city.ac.uk>
Sent: Wednesday, October 28, 2020 7:00 PM
To: Vora, Ankit (A.) <avora3@ford.com>
Subject: Ford AV Dataset - obtaining steering angles
 
Hi,
I am studying your AV datast and have extracted the /imu data from provided .bag files, 
I am trying to work out if the steering angle can be inferred from the data?
Your article states the dataset provides 6DOF pose estimation.
The extracted /imu topic looks like:

header:
  seq: 137596
  stamp:
    secs: 1501822140
    nsecs: 610330104
  frame_id: "imu"
orientation:
  x: 0.00133041249526
  y: 0.00486443211508
  z: 0.584795486661
  w: 0.811165091756
orientation_covariance: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
angular_velocity:
  x: -0.0018081133007
  y: -0.0105949952451
  z: -0.00118502619208
angular_velocity_covariance: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
linear_acceleration:
  x: -0.0136899789795
  y: 0.199539378285
  z: 0.305044800043
linear_acceleration_covariance: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

I guess I am interested in the yaw, which would be the value z, but
having looking at the pattern it does not look like an angle that
could be associated to steering, positive and negative values close
to zero.

Could you help me identify the steering angle, or if it is not
present suggest any approaches to extract it from the data?

Anyway help would be greatly appreciated.

Daniel Sikar
Daniel Sikar | MSc Data Science Candidate
School of Mathematics, Computer Science and Engineering
City University of London   
\end{verbatim}