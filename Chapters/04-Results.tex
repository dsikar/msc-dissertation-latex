
\chapter{Results}
% LC 25pgs
\label{Results} 

%This chapter presents the outputs that you produced, by applying the methods that you have selected, including e.g. analysis, design, prototyping, experimental work, evaluation, etc.  
  
%How you report these results will depend on the nature of the work. It may be helpful to divide them into basic data (e.g., for a project that developed a software product, requirements specification, test data, etc.) and analysis of the data (e.g. statistical analyses, evaluation analyses, etc.). Remember that you are informing the reader of what you have produced and found and emphasising the interesting parts, so summarising at the end of each major section is useful.  
  
%It is usually very helpful for the readers to include graphics and diagrams, for instance to clarify software design or requirements, identify key trends and relationships in empirical data, etc. If you do so, be sure to refer to these figures in the text and use them as evidence to support what you are explaining or arguing; and be sure that your figures are well designed and clearly presented â€“ do not just use default settings of the software you are using in producing them.  
  
%It is essential that you identify clearly what you accomplished or produced yourself, as opposed to what existed before you started your individual project or was provided by others. For instance, some projects build new software on top of an existing code base, add new data to an existing body of data, or are executed by a student as a member of a team. It is essential to indicate what parts of the activities and results which you report are your own work. If this is left unclear, the markers are instructed not to give credit for work that they cannot attribute to you. Ambiguity would attract penalties for poor academic practice, with delays caused by any investigation (deception would be treated as academic misconduct, of course, which may lead to expulsion).

% Feedback from Artur Garcez with respect to Results
% The most important thing now is to describe clearly (with the right level of detail - not too much, not too little) the (...) evaluation of results (there's a more objective evaluation here of accuracy/speed but also a subjective evaluation that you can discuss in more detail: when a situation changes from "no rain" to "rain" and how that affects the pre-trained network). With those results you will have the material needed for the important discussion chapter which should include your own critical evaluation of results and also point to future work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PRODUCING A SELF-DRIVING MODEL
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Producing a self-driving model}

That was hard. Detail the bits that were used from TawnNet (augmentation added) and NaokiNet (crop changed). in the end two outputs were required for NaokiNet to work. Maybe move this to discussion. Here we show results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% EVALUATION OF SELF-DRIVING CARS USING CNNS IN THE RAIN
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Evaluation of self-driving cars using CNNs in the rain}

Once the best models were narrowed down to nvidia1 (\ref{app_res:49}) and nvidia2 (\ref{app_res:62}), a number of test runs were conducted to generate outputs for evaluation. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% OUTPUTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Outputs}
Since this dissertation's topic first took shape (\ref{corr_with_super}) a number of outputs were generated, not all recorded or saved. With respect to outputs that were recorded and saved, this project generated five classes of outputs: 1. models, 2. videos, 3. tcpflow logs, 4. synthetic datasets and 5. code. A number of "Runs" were recorded in \ref{res:training_and_testing_log}. A run may or may not generate a model, may or may not generate a video and may or may not generate a tcpflow log. A model may be run several times, as were the best nvidia1 (\ref{app_res:49}) and nvidia2 (\ref{app_res:62}) models, tested with differente levels of rain and light intensity multipliers.
A compilation of saved models, videos and tcpflow logs, shown in \ref{app_res:outputs} lists 73 models, 61 published (YouTube) videos and 30 tcpflow logs. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SYNTHETIC DATASETS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Synthetic datasets}
The data distribution depends on the track. Figure \ref{fig:GeneratedTrackPlusHist} shows a normalized histogram of 45410 steering angles obtained in 10 recorded sessions (data directory dataset/unity/smallLoop) corresponding to steering angles. The simulator records at a, computational resources allowing, maximum rate of 60 fps (frames per second). The observed average on the track was approximately 24 fps, representing in this case 31m32s  in approximately 19 laps. The 24 fps average was obtained by recording ("Auto Drive w Rec" option) for one lap, taking approximately 1m41s seconds and generating 2455 frames. The simulator registered fps rates higher than average on straight sections and lower than average on curved sections. This is assumed to be due to computational overheads imposed on the physics engine in the curved sections, resulting in less frames being recorded. The maximum steering angle, is set in the simulator with respect to a car. In this case it is plus or minutes 25 degrees. The normalized value, in the range of -1 to 1 is recorded. Values displayed in the histogram are multiplied by the maximum steering angle. The data shows a high bias towards right turns because the simulated vehicle drives clockwise around the track.
\begin{figure}[ht]
 \centering 
 \includegraphics[width=\textwidth]{Figures/GeneratedTrackPlusHistogram.png}
 \caption{Normalized histogram of Unity 3D SDSandbox steering angles for 45410 image frames. The corresponding track (small\_looping\_couse) is shown on the right}
 \label{fig:GeneratedTrackPlusHist}
\end{figure}

Another simulated circuit used was the \textit{Generated Road}, which creates a random path on every run. This can be seen in figure \ref{fig:GeneratedRoadPlusHist}. The total number of frames collected for 15 \textit{Auto Drive w Rec} sessions (saved in dataset/unity/genRoad) was 280727, approximately 3h14m37s. The outliers, mainly to left of histogram, are due to the simulated vehicle being left unattended, reaching the end of the road and continuing into a section with no road markings, becoming stuck in a hard-left turn, until the simulation was switched off and recording halted. The mean and standard deviation, for angles in the range -20 to + 20, are -0.18 and 5.37 degrees respectively. The number of outliers omitted from the plot (not excluded from data at this stage) was 2204 (0.79\%). It can be seen that when more data points are collected, the bell-shaped curve becomes smoother.

\begin{figure}[ht]
 \centering 
 \includegraphics[width=\textwidth]{Figures/GeneratedRoadPlusHistogram.png}
 \caption{Normalized histogram of Unity 3D SDSandbox generated road, steering angles for 280727 image frames. A sample randomly generated road is shown on the right. Outliers is due to oversteering when running off the end of track and sim left recording.}
 \label{fig:GeneratedRoadPlusHist}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NETWORK TRAINING
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Network training and simulator testing}
\label{results:net-training} 
% write up, label runs and make reference
The first working model (able to successfully self-drive around the Generated Track) was   20201107210627\_nvidia1.h5. A video was generated using the recording screen utility Kazam (\cite{Kazam2020}), recording at 15fps (frames per second), and published at  \href{https://youtu.be/9z0mMtOnUUc}{https://youtu.be/9z0mMtOnUUc}. Figure \ref{fig:SimTCPPred}
shows 3 stills from the video containing from left to right, the game engine, the TCP debug output and the prediction engine running.

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{Figures/SimTCPPred.png}
\caption{Stills of video \href{https://youtu.be/9z0mMtOnUUc}{https://youtu.be/9z0mMtOnUUc} showing left to right: SDSandbox simulated car going around the Generated Track course, TCP Debug (tcpflow) and prediction engine (predict\_ client.py) running}
\label{fig:SimTCPPred}
\end{figure}

The video shows the TCP debugging, prediction engine and the game engine simulated car steering with predictions received over the TCP network. The actual code used to generate the model was not logged at the time the experiment log entry was written in the appendix log (\ref{AppendixD}). By verifying git hash commits (4 commits were made on November 7th) as documented in appendix  a second sanity check model \ref{app_res:36} (20201120171015\_ sanity.h5) was created and found to self-drive successfully around the generated track. A video was made from tcpflow frames as described at the end of \ref{app_res:36}, and uploaded to \href{https://youtu.be/JaSkkh-2xtI}{https://youtu.be/JaSkkh-2xtI}.
The video shows (fps discrepancies considered) that the \ref{app_res:36} model had a better lap around the track.
Model 37 (\ref{app_res:37}) trained with genRoad (including outliers seen in Figure \ref{fig:GeneratedRoadPlusHist}) ) did not do well. The oversteering can be seen in video \href{https://youtu.be/xGDN8qOnv9M}{https://youtu.be/xGDN8qOnv9M}. The steering angle bins and graph plots can be seen in  Figures \ref{fig:tcpflow_20201120184912_bins}  and \ref{fig:tcpflow_20201120184912_graph} in the results appendix.

% a number of runs (e.g. commit 2e5bf1b7 failed prematuraly, most failing to generate a model. Doing a diff on one of them shows that batch size was set to 128
% Could this also be an issue?
% Note in original Alexnet which we assume NVIDIA were using as a design reference, used batch size 128 for TWO channels - TODO write-up in Discussion.

%$ git diff master..2e5bf1b7 train.py | grep batch_size
%-    batch_size = conf.batch_size
%+    batch_size = conf.training_batch_size
%(base) simbox@simbox-wifi-server:~/src(master)$ git diff master..2e5bf1b7 conf.py | grep %batch_size
%+training_batch_size = 128
%-batch_size = 64
%+batch_size = 128 # nvidia1 = 64
%@@ -50,7 +49,5 @@ batch_size = 64
Figure \ref{fig:SkewCleanup} shows from left to right, an overlay of all plots of normalized histogram plots for steering values contained in unity/genRoad directory, the folder containing most outliers (logs\_Thu\_Jul\_ \_9\_16\_00\_15\_2020) and the resulting plot once the outliers' folder was removed from unity/genRoad. 

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{Figures/SkewCleanup.png}
\caption{Left to right, normalized histograms of all genRoad folders, outlier's folder and all data with outliers removed.}
\label{fig:SkewCleanup}
\end{figure}

Once the outliers were removed, another model was generated with five epochs of training to test if there was any difference in the loss values observed. The resulting model proved useful and was able to drive along the Generated Road course during a few seconds until the simulation was stopped. Below are the normalized histogram and bins recovered from tcplow logs for model  
  
20201120184912\_sanity.h5 referenced in \ref{app_res:37}. The video generated from tcpflow log was uploaded to \href{https://youtu.be/xGDN8qOnv9M}{https://youtu.be/xGDN8qOnv9M}.

\begin{figure}[ht]
 \centering 
 \includegraphics[width=\textwidth]{Figures/tcpflow_20201120184912_sanity_graph.png}
 \caption{Graph of steering angles recovered from tcpflow log genRoad/tcpflow/20201120184912\_sanity\_tcpflow.log for model 20201120184912\_sanity.h5 driving on Generated Road}
 \label{fig:tcpflow_20201120184912_graph}
\end{figure}

\begin{figure}[ht]
 \centering 
 \includegraphics[width=\textwidth]{Figures/tcpflow_20201120184912_sanity_bins.png}
 \caption{Normalized histogram of tcpflow log genRoad/tcpflow/20201120184912\_ sanity \_ tcpflow.log for model 20201120184912\_ sanity.log driving on Generated Road}
 \label{fig:tcpflow_20201120184912_bins} 
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PRODUCING A SELF-DRIVING MODEL
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Producing a self-driving model}

We added a video of the model running side by side with network as shown in figure 
\ref{fig:20201120171015_sanity_sim_network}, using a procedure as described in \ref{app_res:41}.

\begin{figure}[ht]
 \centering 
 \includegraphics[width=\textwidth]{Figures/20201120171015_sanity_sim_network.png}
 \caption{Video still from \href{https://youtu.be/LEmZJJzJkEE}{https://youtu.be/LEmZJJzJkEE} showing simulator image sent over TCP network on the left with CNN steering angle prediction and processed image (as presented to CNN) on the right}
 \label{fig:20201120171015_sanity_sim_network} 
\end{figure}

% \textbf{STOPPED HERE - NEED TO GENERATE STEERING ANGLE PLOT WITH MEAN AND STD} Looks like we might be picking up the simulator steering angle.  \ref{fig:sa_GeneratedTrack_20201120171015_sanity}
Figure \ref{fig:sa_GeneratedTrack_20201120171015_sanity} shows the predicted steering angles around Generated Track, where the first dip is the sharp right corner, the second dip is the slight right turn followed by a slight left turn plateauing at a negative steering angle (gentle left turn followed by two sharp right turns and finally the straight until the finish line with steering angles close to zero degrees.
\begin{figure}[ht]
 \centering 
 \includegraphics[width=\textwidth]{Figures/sa_GeneratedTrack_20201120171015_sanity.h5.png}
 \caption{Steering angle plot for figure \ref{fig:20201120171015_sanity_sim_network}}
 \label{fig:sa_GeneratedTrack_20201120171015_sanity} 
\end{figure}
% probably best to more this to results section and reference here
%\begin{verbatim}
%# data: genRoad (log2 renamed)
%# commit: 1ad187d4bff5b6936c065a1aaa15a654ef4d368c
%$ python train.py --model=sanity --outdir=../trained_models
%\end{verbatim}
%this will create the a model in trained\_output/sanity/20201120184912\_sanity.h5
% and run as per procedure described in (TODO add reference).
More results, this time as per run 43 (\ref{app_res:43}) 3 images side by side (Figure 
 \ref{fig:tcpflow_Run43}). This was a first attempt to add rain, where the effect is added to the image presented to network. Although the procedure introduces noise to images, it is somewhat unrealistic, as rain is expected to be present on the acquired image. It is left here to document the work development.
 
\begin{figure}[ht]
 \centering 
 \includegraphics[width=\textwidth]{Figures/tcpflow_Run43.png}
 \caption{Video still showing torrential rain added to the image presented to the network \href{https://youtu.be/57jwwcjbfdE}{https://youtu.be/57jwwcjbfdE}}
 \label{fig:tcpflow_Run43} 
\end{figure}
%\begin{verbatim}
%# data: genRoad (log2 renamed)
%# commit: 1ad187d4bff5b6936c065a1aaa15a654ef4d368c
%$ python train.py --model=sanity --outdir=../trained_models
%\end{verbatim}
%this will create the a model in trained\_output/sanity/20201120184912\_sanity.h5
%and run as per procedure described in (TODO add reference).

After the attempt illustrated in Figure \ref{fig:tcpflow_Run43}, the rain-adding workflow was modified such that rain was added to the acquired image, and that image then processed further downstream. Some results can be seen in Figures
\ref{fig:sa_GeneratedTrackintensitymultiplier1_20201207091932_nvidia1}, \ref{fig:sa_GeneratedTrackintensitymultiplier4_20201207091932_nvidia1} and
\ref{fig:sa_GeneratedTrackintensitymultiplier8_20201207091932_nvidia1}.


\begin{figure}[ht]
 \centering 
 \includegraphics[width=\textwidth]{Figures/sa_GeneratedTrackintensitymultiplier1_20201207091932_nvidia1.h5.png}
 \caption{Description}
 \label{fig:sa_GeneratedTrackintensitymultiplier1_20201207091932_nvidia1} 
\end{figure}

\begin{figure}[ht]
 \centering 
 \includegraphics[width=\textwidth]{Figures/sa_GeneratedTrackintensitymultiplier4_20201207091932_nvidia1.h5.png}
 \caption{Description}
 \label{fig:sa_GeneratedTrackintensitymultiplier4_20201207091932_nvidia1} 
\end{figure}

\begin{figure}[ht]
 \centering 
 \includegraphics[width=\textwidth]{Figures/sa_GeneratedTrackintensitymultiplier8_20201207091932_nvidia1.h5.png}
 \caption{Description}
 \label{fig:sa_GeneratedTrackintensitymultiplier8_20201207091932_nvidia1} 
\end{figure}

Figure \ref{fig:youtube20201207091932nvidia1lightrainmult_4_h5} (\url{https://youtu.be/qdTA5ho5VOE}) shows a still containing three images pertaining to the orange line in Figure \ref{fig:sa_GeneratedTrackintensitymultiplier4_20201207091932_nvidia1}. The vehicle steers off the road and fails to turn right.

\begin{figure}[ht]
 \centering 
 \includegraphics[width=\textwidth]{Figures/youtube20201207091932nvidia1lightrainmult_4_h5.png}
 \caption{Still from video \url{https://youtu.be/qdTA5ho5VOE} showing a self-driving simulated vehicle driving off the Generated Track, where the Unity scene is set to a darker horizon (Setting Skybox Material to SkyCarLightCoversGrey) and the intensity multiplier is set to 4. Rain is set to light. The image sent from the network over the TCP network is shown on the left, the middle image has added rain and the image on the right is the image presented to the network.}
 \label{fig:youtube20201207091932nvidia1lightrainmult_4_h5} 
\end{figure}

Figure \ref{fig:youtube20201207091932nvidia1heavy10mult_4_h5} contains a set of stills taken from youtube video \url{https://youtu.be/sKyoke3IO84}
\begin{figure}[ht]
 \centering 
 \includegraphics[width=\textwidth]{Figures/youtube20201207091932nvidia1heavy10mult_4_h5.png}
 \caption{Green line in taken from tcpflow log, the corresponding video \url{https://youtu.be/sKyoke3IO84} referencing figure (TODO REFERENCE)}
 \label{fig:youtube20201207091932nvidia1heavy10mult_4_h5} 
\end{figure}


Figure  \ref{fig:youtube20201207091932nvidia1torrential20mult_4_h5} shows still from youtube video \url{https://youtu.be/mDjtnnVZdic}.
\begin{figure}[ht]
 \centering 
 \includegraphics[width=\textwidth]{Figures/youtube20201207091932nvidia1torrential20mult_4_h5.png}
 \caption{This one has torrential rain (orange line in mult 4 plot), and actually crashes into the first left hand bollard \url{https://youtu.be/mDjtnnVZdic}}
 \label{fig:youtube20201207091932nvidia1torrential20mult_4_h5} 
\end{figure}

Another set of results for the best performing nvidia2 architecture. TODO ADD DISCUSSION, nearly steering off at large negative spike - link to video time, for three videos.  
Figure \ref{fig:sa_GeneratedTrackintensitymultiplier1_20201207192948_nvidia2} shows
\begin{figure}[ht]
 \centering 
 \includegraphics[width=\textwidth]{Figures/sa_GeneratedTrackintensitymultiplier1_20201207192948_nvidia2.h5}
 \caption{nvidia2 20201207192948\_ nvidia2.h5 intensity multiplier 1}
 \label{fig:sa_GeneratedTrackintensitymultiplier1_20201207192948_nvidia2} 
\end{figure}

Figure \ref{fig:sa_GeneratedTrackintensitymultiplier4_20201207192948_nvidia2} shows again three types of rain, light, heavy and torrential.
\begin{figure}[ht]
 \centering 
 \includegraphics[width=\textwidth]{Figures/sa_GeneratedTrackintensitymultiplier4_20201207192948_nvidia2.h5}
 \caption{nvidia2 20201207192948\_ nvidia2.h5 intensity multiplier 4}
 \label{fig:sa_GeneratedTrackintensitymultiplier4_20201207192948_nvidia2} 
\end{figure}

Figure \ref{fig:sa_GeneratedTrackintensitymultiplier8_20201207192948_nvidia2} shows the 3 drives with intensity multiplie (sic) set to 8.
\begin{figure}[ht]
 \centering 
 \includegraphics[width=\textwidth]{Figures/sa_GeneratedTrackintensitymultiplier8_20201207192948_nvidia2.h5}
 \caption{nvidia2 20201207192948\_ nvidia2.h5 intensity multiplier 8}
 \label{fig:sa_GeneratedTrackintensitymultiplier8_20201207192948_nvidia2} 
\end{figure}

\begin{figure}[ht]
 \centering 
 \includegraphics[width=\textwidth]{Figures/youtube20201207192948_nvidia2torrential20mult_8_h5.png}
 \caption{Youtube video still showing the 20201207192948\_ nvidia2.h5 model predictions around Generated Track in torrential +-2- degree slant rain (\url{https://youtu.be/W1eRN5DWPXw})}
 \label{fig:youtube20201207192948_nvidia2torrential20mult_8_h5} 
\end{figure}

% TODO ADD TO RESULTS. Sky was changed for nvidia 1 for all rain runs.

% NB All 6 Generated Track rain/multiplier plots generated with code from commit 610939215 (result_plots.py) 

% Image template
%\begin{figure}[ht]
% \centering 
% \includegraphics[width=\textwidth]{Figures/a}
% \caption{Description}
% \label{fig:} 
%\end{figure}

\subsection{Modifications to original source codes}
Three libraries used:
\begin{itemize}
    \item SDSandbox
    \item Naoki Augmentation
    \item Automold
\end{itemize}
TODO git diff (in appendix) and itemize modifications.
    
Models are trained with code written in train.py, models.py and conf.py. This code has been modified from the original. To compare changes a git diff can be copying the original code over the source code used in this project and performing a "git diff"
\begin{verbatim}
# clone repository for dissertation
$ git clone https://github.com/dsikar/sdsandbox
# clone original
$ git clone https://github.com/tawnkramer/sdsandbox sandbox_orig
# copy original over dissertation
$ cp sandbox_org/src/* sdsandbox/src
# compare
$ cd sdsandbox 
$ git diff
\end{verbatim}


Starting with the NVIDIA baseline, a number of hyperparameters were trialed. The initial setup failed to generate usable models. 
The table below presents training results for best trained models.



Using the baseline neural network architecture as described in 
Models trained with no image pre-processing, did not perform well, leading to cars driving off the road, as shown in Fig.  sequence.

% data gathered on Robot Racing League track
We gathered 10 laps of data on the Robot Racing League track, with maximum speed set to 2.1, proportional control set to 16 and differential set to 77. Maximum steer was set to 25 (degrees). Corresponding to 12778 .jpg image files and the same number of  .json files, containing corresponding throttle and steering angle values recorded at the moment image was saved by simulator. This can be seen in the calls to Update() and SaveCamSensor functions in  
\begin{verbatim}
./Assets/Scripts/Logger.cs
\end{verbatim}

\section{Generated Track performance}


The $G_s$ (goodness-of-steer) model performance score, was obtained by recording one full lap and comparing steering angles with model predictions.

\section{Datasets}

The following training datasets were generated:

Origin  Directory   Number of files Comment
SDSandbox   unity/smallLoopingCourse/log/* 34443 from small\_looping\_course
SDSandbox   unity/warehouse/*   41126 From Warehouse course
SDSandbox   unity/smallLoop/*   45422   From small\_looping\_course
SDSandbox   unity/roboRacingLeague/* 12778 From "Robot Racing League" course
SDSandbox   unity/log\_sample   25791   From small\_looping\_course
SDSandbox   unity/genRoad 280727 From "Generated Road" course



Following the list of data deliverables (\ref{Deliverables-Datasets}) the Udacity data consisting of two files 



%-----------------------------------
%	Network running
%-----------------------------------