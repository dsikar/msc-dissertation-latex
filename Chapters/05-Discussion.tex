
\chapter{Discussion}

\label{Discussion} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Impact on network design and performance
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Good case to be covered between runs 62 (best nvidia model)
% and 70, wobbly but works.


%This chapter examines your results in comparison with your objectives, and then in the wider perspective of other theoretical and applied work relevant to your project, as covered in your review in Chapter 2. For instance, for a software product you will discuss how well it satisfies the user needs that it addresses, its performance and dependability, aspects of design, implementation or assessment that have proved good choices or that instead you would change if you were to repeat the project knowing what you now know. For novel research results or any other knowledge obtained through the project, you will discuss your confidence in the results, their validity, scope and their generalisability. What are the implications of what you have found out? Do you have any recommendations as a result?

%\section{Andrew Ng CNN complexity table - number of parameters}
%Convolutional Neural Network Examples    
  
%See C4W1L10 CNN Example 10m15s  

%Another interesting performance metric, "While ResNets have definitely improved over Oxfordâ€™s VGG models in terms of efficiency, GoogleNet seems to still be more efficient in terms of the accuracy / ms ratio.", basically how much accuracy is achieved with respect to training time, this could provide a single magnitude.

% Discuss Mechanical Turk and Automold images being labeled as No Rain

examine results in comparison with objectives
\begin{itemize}
    \item[--] Game engine - able to support experiments?
    \item[--] Datasets
    \item[--] Image augmentation
    \item[--] Training models
    \item[--] Workflow
    \item[--] Evaluation metric
\end{itemize}
Discuss:
\begin{itemize}
    \item[--] Validity
    \item[--] Scope
    \item[--] Generalisability
    \item[--] Implications
    \item[--] Recommendations
\end{itemize}
Havoc created by outliers in genRoad. 20201206211122\_ nvidia1.h5 performs well on genRoad (where is was trained) but not well on genTrack. Models trained on genTrack also do well on genRoad. Why?
TODO On previous note, add Run 95 - over 16 minute drive \url{https://youtu.be/z9nILq9dQfI}  
Discuss how results in Table \ref{table:goodness-of-steer} fail to indicate numerically that nvidia2 was better than nvidia1 in the rain. A windowing approach may be required, to identify "spikes" to quantify a "crash" likelyhood. The quantity does confirm what has been observed through qualitative analysis, that nvidia2 is a "smoother" drive.