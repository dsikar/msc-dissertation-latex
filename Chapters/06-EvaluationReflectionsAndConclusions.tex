
\chapter{Evaluation, Reflections and Conclusions}

\label{Eval} 

%This chapter should evaluate the project work as a whole. Here the original choice of objectives, the literature examined, the methods used, the planning, etc. are all reviewed to see what has been achieved by undertaking the project. There may be a summary of general conclusions drawn from the work done, highlighting the particular contribution of your project. You should also consider the implications of these conclusions. Discuss any proposals that you might make for further work, having discovered what you now know. It is also important to include a reflective section covering what you have learned from the project process. What would you do differently if you were to start again, knowing what you now know? Your report MUST include adequate Evaluation, Reflections and Conclusions to gain a passing grade.  

%On
%TBC - TODO. Some points to reflect:
%\begin{itemize}
%    \item[--] What this project demonstrated
%    \item[--] What were the limitations
%    \item[--] What have I learnt
%    \item[--] What are the contributions
%    \item[--] What would I have done differently - narrowed the scope for %sure.
%    \item[--] Here would be a good place, perhaps, to discuss "Understanding %deep learning requires rethinking generalization", Zhang ICLR 2017, on deep models fitting random data (noise) perfectly. 
%    \item[--] 
%    \item[--] Future work - add reflections to road. Experiment was limited to adding rain-like effects. With a better understanding of the game engine, it could be possible to add reflections to road, etc, and generate then another set of results and evaluation.
%\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%
% EVALUATION
%%%%%%%%%%%%%%%%%%%%%%%%%%
The choice of objectives, tools and techniques enabled and demonstrated the evaluation of self-driving CNNs in the rain. From that perspective, the overall aims of this project were achieved. 
The main contribution was to create, from pre-existing tools that were successfully integrated and extended, an environment readily available for anyone researching similar topics related to the effect of noise on CNNs, using game engines. 
The general conclusion is rain-like image noise does affect self-driving CNN performance, and the choice of network architecture and image geometry is important to minimise performance degradation. Aspects of network design that help to deal with such noise were successfully identified by careful network design scrutiny. The effect of network architecture, especially geometry of the last convolutional layer feature maps was established, and the impact such design choice has on the quality of self-driving in the rain, generating smoother or jerkier steering patterns, demonstrated. 
This achievement did provide some explainability about network design with respect to predictions.
A metric for quantitative analysis was proposed and proved partially consistent with qualitative analysis. An improved scheme was then proposed.
Network training times and network sizes were investigated, and ensemble models suggested as future work.
Experiments were conducted, documented and outputs carefully annotated. Care was also taken in producing visual displays, be it still or moving images, to help inform decisions. Cross-referencing consistency was a prime concern throughout, especially with respect to the results and discussion. Those are the positive outcomes. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% On the necessity to understand camera properties
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

One aspect that was not accounted for originally, and lacking in this project was a study of camera characteristics that determine geometric properties of the acquired image, demonstrated here to impact network performance on the image size level alone. Important factors not considered include frame of reference, world frame, camera frame, image plane, image frame, intrinsic parameters and extrinsic parameters (\cite{Sala2006}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% On the list of delivered "deliverables"
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

From the original 92 deliverables \textit{deliverables} outlined in \ref{AppendixC-deliverables} this project managed to deliver D1, D2 and D4 datasets. D3 was not attempted (as over time came the awareness that it may not be good practice to mix datasets, later confirmed with understanding of image geometry and its impact on network architecture. All simulator testing tracks S1, S2 and S3. Model N1 (PilotNet), plus deliverable IDs 13 through 16. Completing a total of 11 out of 92. A total closer to the original goal would have been achieved, had the AlexNet, VGGNet, InceptionNet/GoogleLeNet and ResNet models been adapted for self-driving, which in hindsight does not seem realistic, hence the original goal was overambitious.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% On the importance of human readable and self documenting naming conventions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The naming conventions used in throughout this project, for data directories, models, tcpflow logs, etc, could have benefited from some time invested in devising a more human-readable and self-documenting scheme, as well as code documentation. An effort was made to this end while the research work was being carried out. Attempts to improve experiment documentation can be seen in \ref{res:training_and_testing_log} where the level of detail and layout increase over time. Still it was laborious sometimes to recall an experiment's setup and results, which could be addressed with additional planning.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% On transforming deep regression problems into deep classification problems
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Future work}
%\subsection{Attaining deep regression with deep classification}
Although CNNs are not well understood, they are heavily used. A lot of effort has gone into creating models originally designed for image classification. Future work could determine if alternative deep models (ResNet, LeNet, VGGNet) could be used for self-driving. These models have been applied successfully to multi-class classification problems. Assuming the network design somehow is optimized for this type of task, it would be interesting to transform regression problems into multi-class classification problems to make optimal use of such networks. This could be attained by quantizing (assigning a continuous value within a defined range to a discrete value) and binning the outputs, subject to the quantized values having acceptable steering precision, in the case of a self-driving car, the minimum acceptable steering change. The network output would become discrete, and the network still leveraged as a classifier - the original intent. Perhaps such scheme could also produce usable results for other computer vision applications relying on regression models and wishing to use state-of-the-art classification models.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Reflections on change of Work Plan
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The work plan originally set out in appendix \ref{app:rpmi} was changed and an estimate of the revisions are shown in Figure \ref{fig:Revised-work-breakdown-structure}. In hindsight, the original plan seems to reflect a \textit{waterfall} (\cite{balaji2012waterfall}) software development model, where all requirements are defined at the outset, the work then separated into logical blocks and sequenced following a delivery schedule. In practice, many (as opposed to the originally expected few) areas were found to overlap such as selecting (and creating) toolsets, replicating existing and creating alternative models, augmenting data and evaluating models, while creating metrics. The end of October and beginning of November 2020 task overlap reflect the busiest research period, when a working model, qualitative evaluation toolset and a quantitative evaluation metric were obtained. The work followed what could be described as an \textit{agile} process, with short \textit{sprints} and concurrent changes reflected across several tasks. Still, the work plan was essential is conceptualising and structuring the effort needed to complete this project.
Ultimately the contributions in explaining network design and self-driving, and evaluation provided in this study could help make self-driving vehicles relying on CNNs in the rain safer. 






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Machine Learning, AI and social responsibility
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% moved to context
%The issue of artificial intelligence and social responsibility (\cite{saveliev2020artificial}) perhaps should also be considered, as developments in self-driving AI may reduce both demand for drivers and wages.

%The absence of good quality labelled datasets (maybe change this w.r.t. Mech Turk and ISLSVRC - "the presence of a good quality" labelled dataset engered a number of landmark computer vision CNN architectures, more obvious and less citations required) has been cited (TODO citation needed) as a hindrance to the development of good models (TODO need to insert in CONTEXT section "the term model hereafter, refers to a either a neural network architeture, or a trained model capable of making predictions given an input). It is ironic that the availability of good quality labelled models, such as Imagenet, led to the improvement in image classification models, which in turn may curb the need to use Mechanical Turk. This could perhaps form part of the wider discussion on automated workforce replacing human workforce and the social implications therein.  


%% Ethical issues - maybe leave to discussion and further work
% This also raises issues related to safety, liability, privacy, cybersecurity, and industry %risks \cite{Taeihagh_2018}
% Also, perhaps a discussion on developing systems that may put people out of work and social responsibility.

% to process more datasets, the maximum steering angle of vehicle must be known

% Reflection on the importance of naming conventions, over which some hours were spent
% to disambiguated runs. No sure what the best solution is here, but looks like 
% current scheme of YYYYMMDDHHMMSS_MODEL.h5 does not easily discriminate values as
% for instance RUN_XX_MODEL.h5 might have done. Even though models are repeated across
% runs, so that is also not ideal. Need to mention \label{app_res:62} and the 10 added 
% hidden units

% Reflect on the fact that feature maps for Alexnet could not be calculated according
% to Dumoulin and Visin, 2018 (Eq. 3.3). This should for part of a broader discussion
% stating the experience that models, be it Alexnet or NVIDIA, are not documented well
% enough to the point of being reproducible.

% Reflect on different hues generated by converting tcpflow packets into images. This differs from direct processing of Unity .jpg synthetically generated data.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SUBMISSION CHECKLIST
% 1. This pdf                                                       MONDAY
% NB COMMENT IN ALL APPENDIXES BEFORE FINAL COMPILATION
% 1.1 Readme.txt - shared links plus setup instructions             MONDAY - SUBMIT WITH PDF
% 2. msc-dissertation-latex repository                              MONDAY - Commit code and get shareable link
% 3. SDSANDBOX repository                                           Uploading to OneDrive
% 4. msc-data repository - CANCELLED - only using unity data  
% 5. Additional data - models, tcpflow logs, etc
% 5.1 Models, tcpflow logs, training logs  - trained_models.tar.gz  SHARED LINK:
% https://cityuni-my.sharepoint.com/:u:/g/personal/daniel_sikar_city_ac_uk/ERt1rJCR3bNApjGumrKXlA4B6SzwutzOtpQH6egT_Vl6cw?e=za5e7p
% 5.2 Synthetic datasets                                            SHARED LINK:
% https://cityuni-my.sharepoint.com/:u:/g/personal/daniel_sikar_city_ac_uk/Eb-C0jivxuNPjOa2CthUGLIB3O2tvcZWQDw_6FqkHUc4Lw?e=mh19Tu
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%. Models and tcpflow data
% $ tar cvf  trained_models.tar.gz trained_models/nvidia_baseline/ trained_models/nvidia1 trained_models/nvidia2 trained_models/sanity/
% $ ls -lsh trained_models.tar.gz 
% 964M -rw-r--r-- 1 simbox simbox 964M Dec 17 17:38 trained_models.tar.gz

% Synthetic datasets
% $ tar cfv dataset.tar.gz dataset/unity/genRoad dataset/unity/genTrack dataset/unity/log_sample/ dataset/unity/smallLoop
% $ ls -lsh dataset.tar.gz 
% 1.6G -rw-r--r-- 1 simbox simbox 1.6G Dec 17 18:02 dataset.tar.gz

% source code
% $ tar cvf sdsandbox.tar.gz sdsandbox/
% $ ls -lsh sdsandbox.tar.gz
% 1.8G -rw-r--r-- 1 simbox simbox 1.8G Dec 17 18:57 sdsandbox.tar.gz

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% README.TXT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% 1. Download source code via brownser
% https://cityuni-my.sharepoint.com/:u:/g/personal/daniel_sikar_city_ac_uk/ERQ3EbK5Z9BAmCDPlw64yo8BmYkQ7TcEA4TTUz1hJU3lJg?e=HbZB1Z

% 2. Download trained models via brownser
% https://cityuni-my.sharepoint.com/:u:/g/personal/daniel_sikar_city_ac_uk/ERt1rJCR3bNApjGumrKXlA4B6SzwutzOtpQH6egT_Vl6cw?e=za5e7p

% 3. Download datasets - for training additional models via brownser
% https://cityuni-my.sharepoint.com/:u:/g/personal/daniel_sikar_city_ac_uk/Eb-C0jivxuNPjOa2CthUGLIB3O2tvcZWQDw_6FqkHUc4Lw?e=mh19Tu

% Unpack files, moving 2. and 3. inside 1. That create the directory structure used to train 
% test code.
% To run tests, install Unity. To train, follow procedure in SDSandbox/src/readme.txt

% TODO PDF with source code modifications to be added in appendix