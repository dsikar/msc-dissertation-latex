01.08.2020

Reread the MSc Project Guidance Document (https://moodle.city.ac.uk/mod/resource/view.php?id=1484635) and a project report given as sample reference (Minah 2015 - https://moodle.city.ac.uk/mod/resource/view.php?id=1484742)

Examined audi dataset preview from https://www.a2d2.audi/a2d2/en/download.html trying to work out how to map images to steering angles. Have been in touch with Dr. Mahmud, on the the authors of Audi papers with respect to aforementioned mapping.

Managed ok. Timestamp is in TIA format. For every .png image there is a corresponding .json file with image timestamp information. All steering angles are provided in a single file containing bus signals, such that the reading taken at the time nearest to a given image may be considered the desired steering angle. A few random images were examined, consisting of urban scenes and not adequate as the vehicle is driving through a city circuit, where no inferences are made about vision, road geometry and steering.

03.08.2020

Downloading Ford Dataset - need to unpack and look at rosbag data to infer steering angles (algorithm to be written). 
Amazon mechanical turk taps into this data, to find rainy sections.
See https://avdata.ford.com/downloads/default.aspx

Structuring report Introduction and Objective based on Chammas 2019, using wordcounts and reference counts as a base.

We need to make a start asap on the Unity testing circuits (x3), that will effectively generate our data. We also need to make a start asap on the NVIDIA vanilla model, be it in PyTorch or Keras/Tensorflow, and get training and testing this model while we develop the alternative models.

Good-to-have would be an AWS training/testing instance, depending on costs.

04.08.2020

Replied to email from supervisor, suggesting a write-up be made of preliminary results, which may be re-used in report (thesis). I'll aim to get that over by end of August. A lot to do i.e. create tracks, create models generate data (see next entry).

Watched Tawn Kramer's intro video to Unity x Tensorflow self driving setup. Model trained on 100 GB of data - https://www.youtube.com/watch?v=e0AFMilaeMI

Flow chart, diagram generating software. I am looking, considering Gimp, Inkscape. Also found Dia, which looks promissing.

25.08.2020

** Ran on both camber and devcloud:

To generate models.

1. clone git repo:

$ git clone https://github.com/dsikar/sdsandbox.git

2. Copy data

$ scp devcloud:~/git/sdsandbox/dataset/autogen_track.tar.gz

3. Unpack data to ~/git/sdsandbox/dataset/log

4. Change directory to src and run

$ python train.py --model=../outputs/camber_model_4.h5

Notes:

Camber fails with error "Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled"

** Our main task w.r.t. models is adapt additional models (i.e. in addition to NVIDIA model) to become regression models, as the non-NVIDIA architectures are defined as classification models where the last layer will tipically be a 1000-way output (with softmax?) drawing the "winner" from a distribution (discuss).

We found this article lathuilire2018comprehensive (added to dissertation.bib) with a discussion about deep regression models.

28/29.08.202

** Still ruminating over GoogleLeNet article. Pressing issue now is get models trained and tested on basic setup, i.e., Unity 3D generated images and steering angles. we have a head start in Tawn Kraemer's code (train.py) and other implementations.  

The link trail:

One Udacity alumni:
https://towardsdatascience.com/deep-learning-for-self-driving-cars-7f198ef4cfa2
did a good write-up, reported problems and cites this repo as the salvation:
https://github.com/hminle/car-behavioral-cloning-with-pytorch/
who in turn was inspired by this repo:
https://github.com/naokishibuya/car-behavioral-cloning
Naoki did a good write-up here:
https://medium.com/@naokishibuya/introduction-to-udacity-self-driving-car-simulator-4d78198d301d

Tasks (one of two, better the two, but at least one):
1. Get the unity 3D data we have to work with naoki's model - medium difficulty
2. Get Naoki's model to work with Tawn's setup - better but potentially hard, as a large library needs to travel across.
https://github.com/tawnkramer/sdsandbox
Let's start with 1. ... 21:26...

We need to check if image sizes are compatible, or what kind of adjustment needs to be made...

Later, need to assess what is the minimum steering angle required, and the quantisation, i.e. precision. Currently angle tipically is 0.09035701304674149

Tawn Kraemer angle in radians	Angle in degrees
0.09035701304674149		5.1770754969996453099

If we use 0.09, we get 5.15662, which is pretty dawn close.
If we use 0.1, we get 5.72958. Which is not ideal but gives
us an idea of what we could do with binning these values.

We will need to look at angle values as a whole, to decide. 
The hypothesis is that angles will not vary a hell of a lot.

Naoki's model seems to spend most of the time between +- 2 degrees.
going as far as +-8 degrees, with two decimal placess, so about 1600
discrete values, though that could probably be reduced a lot, if say
we allow changes in 1/3 of a degree, that would be 48 values total,
with the majority around 12 values (4 * 3 ~ +- 2).

So basically we would be looking at converting and truncating, i.e.
convert 0.09035701304674149 to 5.1770754969996453099 and
truncate 5.17, or round, TBD.

And jumping ahead, here is a tutorial on adding rain to Unity 3D through "particles"

https://www.youtube.com/watch?v=Ax3WNI-3C60

Another good one, maybe better - simpler, parts 1 and 2
https://www.youtube.com/watch?v=VN6RzRQ-SWU
https://www.youtube.com/watch?v=8inqJf4aNH0

Some pointers
https://www.youtube.com/watch?v=YHHVCqoNuEY

29/30/31.08.2020

We looked at Andrew Ng's videos on ConvNets and also read and annotated the GoogleLeNet (Inception) paper. 

We now have under 3 months to deliver first draft and under 4 months to deliver dissertation. We need to start thinking about downsizing project if we do not get some results by the end of this month, e.g. one or two nets only, no evaluation on circuit, just on steering angles.

01.09.2020

We looked at the VGG paper and wrote some notes.

06.09.2020

We tried to install tensorflow and keras on RPI zero W without success, so trying to compile from source. Also tried on Ubuntu 16.04 without success, both procedures as outlines in:

https://www.tensorflow.org/lite/guide/build_rpi

Notes on the AWS Ubuntu build. We had to allocate extra space to the volume (up from 8 to 20GB) then on the instance (t2.micro):

   42  lsblk
   43  sudo growpart /dev/xvda 1
   44  lsblk
   45  df -h
   46  sudo resize2fs /dev/xvda1
   47  df -h

We used a docker container:

   22  curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
   23  sudo apt-key fingerprint 0EBFCD88
   24  sudo add-apt-repository    "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   25     $(lsb_release -cs) \
   26     stable"
   27  sudo apt-get update
   28  sudo apt-get install docker-ce docker-ce-cli containerd.io
   29  apt-cache madison docker-ce
   30  sudo docker run hello-world
   31  sudo systemctl status docker
   32  sudo usermod -aG docker ubuntu

Downloaded the recommended image:

   50  docker pull tensorflow/tensorflow:latest-devel

then ran docker:

   57  docker run -it tensorflow/tensorflow:latest-devel

We tried to cross-compile in the docker image (is this the right jargon) and ran into problems.

NB to exit docker:

$ exit

To get shell of running container:

$ docker exec -it <mycontainer> bash

To list images:

$ docker images

To list running images:

$ docker ps -a

To commit:

$ docker commit <image id> name (I think)

To kill a running image:

$ docker kill <image id>

Some online notes on docker:

https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-18-04

Wrt cross-compiling tensorflow lite, this link looks promissing:
https://www.tensorflow.org/install/source_rpi#python-3.7

Started running on EC2 instance but ran out of space, having already increased volume size under 6 hours ago so fell foul of AWS rules and had to halt for the night.

Need now, once more space is allocated, rerun from tensorflow_src directory:

$ tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 tensorflow/tools/ci_build/pi/build_raspberry_pi.sh PI_ONE

Also, this linked page:
https://www.tensorflow.org/install/pip

Had a wheel I don't think I've tried, so maybe tomorrow:

Raspberry PI (CPU-only)
Python 3, Pi0 or Pi1	https://storage.googleapis.com/tensorflow/raspberrypi/tensorflow-2.3.0-cp35-none-linux_armv6l.whl

07.09.2020

Trying again to build a tensorflow wheel for RPI Zero. Running t3.medium (4GB memory) with 40GB.

Installed as previous log entry, starting with:

$ sudo apt-get update
$ sudo apt-get install build-essential

Then docker as per notes, then cloned tensorflow into tensorflow_src, and from that directory ran:

$ sudo tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 \
> tensorflow/tools/ci_build/pi/build_raspberry_pi.sh PI_ONE

Previous install from this morning got stuck and bricked instance, so starting again. Failing this, we will look at building a Ubuntu 16.04 cross compiler box tomorrow and taking it from there.

Added a virtual environment to the pi zero as per:
https://www.tensorflow.org/install/pip#raspberry-pi

Note, when finished:

(venv) $ deactivate  # don't exit until you're done using TensorFlow

Inside the virtual environment we then ran:

wget https://github.com/lhelontra/tensorflow-on-arm/releases/download/v2.2.0/tensorflow-2.2.0-cp37-none-linux_armv6l.whl

And then

$ pip install tensorflow-2.2.0-cp37-none-linux_armv6l.whl

That installed tensorflow ok, keras also in, (about 12 hours' work), though both are very slow. TODO read Adrian Rosenbrook's notes on increasing virtual memory or something like that. We are stopping the ubuntu build but might still be worth pursuing tensorflow lite build for inference only.

TODO Run tensorflow lite inference to see if it flies. DONE! Inference ran ok on raspberry pi zero w. Need to document plus address previous TODO.

08.09.2020

Failed build on Ubuntu 18.04 tensorflow branch 2.3 and master:

1. build-essential
2. Installed docker
3. Cloned tensorflow
4. Ran sudo tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 tensorflow/tools/ci_build/pi/build_raspberry_pi.sh PI_ONE

** Tried to build tensorflow lite...

https://www.tensorflow.org/lite/guide/build_rpi

Failed. As things stand, we were unable to cross compile on 1. AWS, 2. 16 core, 32 GB memory Ubuntu 18.04 and natively on RPI Zero. TODO try with Ubuntu 16.04 as this is referenced in the docs. DONE Tried 16.04 AMI, no luck.

Looks like we will not be able to run tensorflow lite on pi zero:

There are some examples on tensorflow website for (presumed) desktop:

# loading 
https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_python
# running
https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/raspberry_pi

12.09.2020

Big TODO left from this week, NEED to document tensorflow running on pi zero w.

Tasks for the day:

1. Read up on the donkey car simulator (Tawn Kramer) as documented on Donkey Car:
http://docs.donkeycar.com/guide/simulator/

2. Look at building a dev machine on AWS. NB We have a physical machine (Ubuntu 18.04) but could do with a cloud version for agility.

There is prior work on Ubuntu/Unity/EC2/GPU in:
https://towardsdatascience.com/how-to-run-unity-on-amazon-cloud-or-without-monitor-3c10ce022639

3. Read up on Amazon SageMaker group truth w.r.t. labelling rainy/dry images from self-driving datasets.

Week beginning 14.09.2020

1. Looks into ROS (robot operating system) and unpacking steering angles from Udacity dataset

2. Look at implementing Clear Grasp algorithm - unrelated to dissertation but some overlap with ROS which may be synergetic.

14.09.2020

1. Installed Ubuntu Desktop on AWS EC2 Instance following:
https://ubuntu.com/tutorials/ubuntu-desktop-aws#5-connecting-to-ubuntu-desktop

23.09.2020

Found article on deep regression, as this is one approach we could use - the other being binning steering angles (classification) and continuing to use same models with fewer classes, which could prove easier to implement.

https://arxiv.org/pdf/1803.08450.pdf

2020.10.07

0817h - 0917h session

Looking into rosbag file formats, downloading udacity data and processing data such that each frame will have a known steering angle.

We had previously downloaded all torrents as listed on the udacity github page:
https://github.com/udacity/self-driving-car/tree/master/datasets

We decompressed and examined one torrent file. Once decompressed, we found a center/ directory with images named after the frame ids e.g. <frame id>.jpg, a final_example.csv with frame ids and steering angles in a comma separated file and a HMB_3_release.bag (assumed) rosbag file - to be studied.

$ tar xvf Ch2_001.tar.gz
$ head final_example.csv 
$ ls center/ | wc -l
$ at final_example.csv | wc -l

Dealing with rosbag files:
http://wiki.ros.org/Bags
http://wiki.ros.org/ROS/Tutorials/Recording%20and%20playing%20back%20data
http://wiki.ros.org/rosbag/Commandline
http://wiki.ros.org/rosbag/Tutorials/reading%20msgs%20from%20a%20bag%20file

2020.10.08

0820h - 0932h session

Installed ROS Noetic following:
http://wiki.ros.org/Installation/Ubuntu

NB installed "melodic" distro:
$ sudo apt install ros-melodic-desktop-full

Installed ok. Started tutorial
http://wiki.ros.org/ROS/Tutorials/Recording%20and%20playing%20back%20data

Stopped at:
2.1 Recording all published topics

1545h -1616 session

Recorded and played back ok. Now looking at:
http://wiki.ros.org/ROS/Tutorials/reading%20msgs%20from%20a%20bag%20file

2020.10.09 

0930h - 1034h session

Continuing with reading messages, we are examining the bag file contained in Ch2_001.tar.gz

wiki.ros.org - domain name expired 10:01h

Looking at NVIDIA CNN with udacity images and steering angles

Repo https://github.com/tawnkramer/sdsandbox.git

Log files logged in sdsim/log. Each image file has a corresponding json file, where image is named e.g. 49063_cam-image_array_.jpg and corresponding json file record_49063.json.

In json file we have image name attribute ("cam/image_array" - matching image) and "user/angle". These files are generated by the Unity application. 

Udacity dataset has the labelled images plus a final_example.csv with image ids and steering angles, in comma-separated files. 

To adapt the workflow we could either rename all files and create json files for each corresponding images, then run prepare_data.py on modified/created Udacity data files (seems harder) or, edit train.py to parse final_examples.csv (seems easier). 


2020.10.10

Running sdsandbox script with Udacity data on Intel cloud

We noticed that data generated by the unity sim, in addition to .jpg and .json files for every frame, also includes log_car_controls.txt and meta.json files for every collected batch.
log_car_controls.txt is empty (possibly the artifact of a flag e.g. "log controls" flag) and meta.json contains the structure of the .json file.

2020.10.11

We reran unity 3d train.py (https://github.com/dsikar/sdsandbox.git) from master version, this took over 10 hours to run on intel cloud

########################################################################
#      Date:           Sat Oct 10 08:49:07 PDT 2020
#    Job ID:           708745.v-qsvr-1.aidevcloud
#      User:           u44986
# Resources:           neednodes=1:gpu:ppn=2,nodes=1:gpu:ppn=2,walltime=23:59:59
########################################################################


########################################################################
# End of output for job 708745.v-qsvr-1.aidevcloud
# Date: Sat Oct 10 19:33:21 PDT 2020
########################################################################

Dataset: autogen_track.tar.gz consisting of 15 recorded Unity 3D ciruit logs (images, throttle and steering angle values). Decompressed size is 2.2G

u44986@login-2:~/git/sdsandbox/src$ du -sh ../dataset/log
2.2G    ../dataset/log

Added single prediction code in helper_functions.py. Some results:

    predict_sa('../outputs/intel_model_qsub_v2.h5', '../dataset/log/logs_Fri_Jul_10_09_16_18_2020/10000_cam-image_array_.jpg')
    0.105303116
    cat ../dataset/log/logs_Fri_Jul_10_09_16_18_2020/record_10000.json
    "user/angle":0.09035701304674149,

    predict_sa('../outputs/intel_model_qsub_v2.h5', '../dataset/log/logs_Fri_Jul_10_09_16_18_2020/10100_cam-image_array_.jpg')
    -0.12858404
    cat ../dataset/log/logs_Fri_Jul_10_09_16_18_2020/record_10100.json
    "user/angle":-0.13314887881278993

    predict_sa('../outputs/intel_model_qsub_v2.h5', '../dataset/log/logs_Fri_Jul_10_09_16_18_2020/10400_cam-image_array_.jpg')
    0.081949875
    cat ../dataset/log/logs_Fri_Jul_10_09_16_18_2020/record_10400.json
    "user/angle":0.07557757198810578,


2020.10.12

Trained model ok on Udacity data. Preliminary results are not as good. Factors that come to mind are quantity of data, noise (cars, threes, shadows, etc) if compared to Unity data, which is visually much cleaner (road, landscape, sky, shadow of vehicle occasionally). ssh 
Main advancement is we can change datasets.

Note, history must be saved after fitting the model - to keep tabs on accuracy and loss.

Prelimary results:

    ** Udacity
    predict_sa('../outputs/udacity1_intel_qsub.h5', '../dataset/udacity/Ch2_001/center/1479425472688106000.jpg')
    0.0012489989
    cat ../dataset/udacity/Ch2_001/final_example.csv | grep 1479425472688106000
    1479425472688106000,-0.0922870745882392

    predict_sa('../outputs/udacity1_intel_qsub.h5', '../dataset/udacity/Ch2_001/center/1479425452434545378.jpg')
    -0.0057435557
    cat ../dataset/udacity/Ch2_001/final_example.csv | grep 1479425452434545378
    1479425452434545378,-0.336740952916443

    predict_sa('../outputs/udacity1_intel_qsub.h5', '../dataset/udacity/Ch2_001/center/1479425441182877835.jpg')
    -0.020317154
    cat ../dataset/udacity/Ch2_001/final_example.csv | grep 1479425441182877835
    1479425441182877835,-0.373665106110275

Rerunning training to save history. Preliminary conclusion, need A LOT more data to train this model. Hypothesis is current architecture is overdimensioned (too many parameters for not enough data). 

Rerunning unity models on both camber and devcloud to compare training times on both platforms.

Job run on camber - africa node:
started job: 201012.162307.266
finished job: 201012.184213.014
2h20m approx. to complete

2020.10.12

When dealing with adapting the neural network architectures to a regression problem, another possibility that could be examined is binning the steering angles, thus not changing the classification nature of the classic image classification deep neural networks.

2020.10.16

Ideally should find a set of metrics similar to "A Survey on Performance Metrics for Object-Detection Algorithms." and applied to self driving.


2020.10.17

Need to pull our socks up now, just over two months to go, we are behind schedule. On the menu for tomorrow, set download job on camber and devcloud to download udacity data available via HTTP links.

2020.10.18

Downloaded everything that could be downloaded to camber, minus Ford TBC and Udacity torrents TBC. 
Got a few citations (about 20 over the weekend), mainly covering Context section - other works I have seen would have from 40 to 60 citations.
Now looking at the models, need to get these work for regression, or use classification and binning, with bins covering ranges. This could be a "major contribution" of this work, if the models show any gains in training and/or predicting over deep learning regression models.
This is a reference of implementing deep networks with 

Keras:https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html

Before going to bed, we need to get a couple of jobs running, one on devcloud (Unity3D data), one on camber (Udacity real life data), and save the history.

We need to start making note of every model run. Something in the lines of
Environment Network Data    Output model    Output History  Date/Time (UTC)     Training time
Devcloud    NVIDIA  Unity3D dc-nvidia-u3d   001-history     2020-10-18 2117h    10h23m
Camber      NVIDIA  Udc-1   dc-nvidia-u3d   002-history     2020-10-18 2125h    5h23m

Also, need to start saving models and history to google cloud, and make available to examiners.

2020.10.19

TODOS 
1. Finish downloading Udacity data
2. Download Ford AV dataset

We have not finished the Udacity data, so that will rollover to the next week. Ford AV dataset is big, with one sample compressed file, and 24 compressed files of interest, decompressing to 12 image directories and 12 rosbag files, from which we should be able to extract steering angle as, according to authors, "in our data set we provide the 6-DOF pose estimates obtained by integrating the acceleration and velocity.".

For the spare hours during the week focus should be on describing datasets, and trying to establish if steering angle can be obtained from all. We have determined that is the case for Udacity/Unity3D (synthetic), Udacity, Audi and Ford. So Kitti needs to be confirmed. 

Confirmed, kitti files do have yaw, the heading in radians, which raises the important point that all angle values must be in the same measurement.

Also, we need to structure the data repository. One possibility is have it as a separate repo, with all data files ignored, and only the download scripts committed. Such that once cloned, scripts can be run and data downloaded. Then symlink created to code directory so models may be trained.

/msc--data
    README 
    /udacity
        download_script.sh
        README
        # any further scripts e.g. extract steering angles
    /unity
    /audi
    /ford
    /kitti
    /mechanical-turk

$ ln -s ~/git/msc-data/ data

2020.10.20

Create template download directory
$ git clone https://github.com/dsikar/msc-dissertation-data.git

2020.10.21

Set up new Ubuntu 20.04 workstation

2020.10.22 

Downloading datasets to new workstation

This should give us 3 environments with reasonable bandwidth to work with. Home environment is low bandwith and now good for downloading data.

2020.10.24

Data tidy up day. Creating download directory structure on camber and devcloud. Downloading ROSBAG file with IMU steering angle equivalent for Ford.

The sample rosbag was extracted ok from .tar.gz file. Two other compressed files from the V2 dataset were extracted, resulting in errors similar to:

1501819953297253.png
tar: Unexpected EOF in archive
tar: Unexpected EOF in archive
tar: Error is not recoverable: exiting now

The assumption is because this was a multi-file download, as soon as the quota was exceeded, all downloads were interrupted. Another attempt will be made for a single file.

Persisting issues when downloading on Intel DevCloud, while decompressing:

$ tar xvf 2017-08-04-V2-Log1-Center.tar.gz

(...)
1501819952451885.png
1501819952620940.png
1501819952790009.png
1501819952959089.png
tar: Unexpected EOF in archive
tar: Unexpected EOF in archive

Comparing files extracted from two download attempts:

$ ls -l *.png | wc -l
1996
$ ls -ls ~/git/sdsandbox/dataset/ford/2017-08-04-V2-Log1-Center/*.png | wc -l
1974

It looks like scheduler is stopping downloads. Another environment may be required if issue persists.

A script to extract images from bag files has been added to repository. The steering angles for sample data are not a good example as the road is straight with no turns taken, meaning the data will be unbalanced. Some augmentation may be required to balance the data, when roads that are not straight are found.

2020-10-26

Internet connection improved. Starting local downloads.

%% UNITY

Looked at some unity sim projects here:
https://github.com/udacity/self-driving-car-sim

Started the unity engine:
$ cd 
$ ./unity_start

Project does not load. Looked at version:
~/git/self-driving-car-sim/ProjectSettings(master)$ cat ProjectVersion.txt 
m_EditorVersion: 5.5.1f1

This was changed to 
m_EditorVersion: 2019.4.2f1
m_EditorVersionWithRevision: 2019.4.2f1 (20b4642a3455)

To load project:
$ ps aux | grep unity

Kill all unity processes.

Start the unity image as sudo
sudo ./UnityHub.AppImage --no-sandbox

Load self-driving-car-sim 
Change unity version to 2019.4.2f1

To load scenes:

Assets > Scenes > MenuScene

Thereafter, unity can be started with 
$ ./unity_start

To add more projects, run UnityHub.AppImage as described above.

Leaving Unity on this footing, further investigation into linux logs would be good to find out about these silent errors when loading new projects without sudo.

Rest of the day tidying up download structure. Also, found this link
https://towardsdatascience.com/how-to-train-your-self-driving-car-using-deep-learning-ce8ff76119cb
Would a good example on binning the steering angles, showing how most data is "straight", so data is unbalanced. This needs to be addressed.

Next, we need to run the training as described here:
https://github.com/naokishibuya/car-behavioral-cloning
which has good data augmentation
Plus some share experience from someone who implemented several architectures:
https://github.com/paul-o-alto/keras-resnet-sdc

2020-10-27

Development box 1 reaching capacity, all data except audi to be removed, except sample to test functions. Scenario will probably be one dedicated box per dataset.

Current Audi download with about 40G to download

$ du -sh *
177M	camera_lidar-20190401121727_bus_signals.tar
64G	camera_lidar-20190401121727_camera_frontcenter.tar
74G	camera_lidar-20190401145936_camera_frontcenter.tar
51G	gaimersheim
4.0K	local-download.sh

Plus Ingolstadt data 200M total Audi size, compressed and decompressed so far is 230G.

So devbox1 is the audi-box from now on. Stopped on binning task.

2020-10-28

Need to take a week off before fast approaching first draft deadline. Also, might have to drop Ford dataset, as steering angles will be an issue. So far we have steering angles in udacity, unity and audi. Ford and kitti are the unknowns as far as extracting steering angles. 
So approach is work with existing steering angles to have partial results for first draft, then try to incorporate ford and kitti between 1st and 2nd draft.

2020-10-31

Running training/testing. 
Starting with NVIDIA architecture, unity data.

Results table

Run ID  Data    Network Training    Acc Error   Saved Best Model    Saved History
1       Udacity 1       SGD Adam    .95 .06     model_01            history_01

Automation is required. Let's start by  creating a smaller dataset that will train in minutes, for the sake of establishing the workflow.

Sample has 12895 .jpg files and 12895 .json files with steering angles.

$ ls -lA logs_Mon_Jul_13_08_29_01_2020/ | wc -l
25791


2020.11.01

A good degree of automation was achieved. We just need to add some extras, like training rate, error function, epochs, etc. Plus ideally the network architecture, which we can recover from model, but best have it on a plain text file for ready access, plus the git branch and commit hash for reproducibility.

To re-use Naoki's code, we need to generate a "driving log" file, such as:

center,left,right,steering,throttle,brake,speed
IMG/center_2016_12_01_13_30_48_287.jpg,IMG/left_2016_12_01_13_30_48_287.jpg,IMG/right_2016_12_01_13_30_48_287.jpg,0,0,0,22.14829

that only includes center and steering. We got that, code does not run as environments are incompatible with TK's. We are installing now installing Naoki's environment with conda.

2020.11.02

Conda install failed. So far no success running Naoki's code. Starting to document training sessions in Appendix D initially. 

Next todo: .tex file to work on individual sections, without need to recompile whole document.

Also, need to record the figure of 8 track data, as the data we have might be a bit too random for the model size.

Also, need to recoup models from stale jobs and carry on training, as one job overran the 24h limit on camber and stopped.

2020.11.07

After running a number of training runs (ids 1 to 10), the accuracy is not going beyond 0.87. This is presumed unsuitable to simulator self-driving, as a previous unity self-driving simulator proof-of-concept run using models producing such accuracies, failed to keep on tracks corresponding to log2 (used in run ids 1 to 10), that is random tracks which are created at every recording run. 
Another track that was used during the proof-of-concept phase was the circular "jungle" track, where self-driving was also not achieved. In retrospect, there was very little data (one or two laps, if memory serves me right) so the next experiment will be generate more several laps of "PID" self-driving data, train a model with such data, with three options, no pre-processing, no data augmentation, only pre-processing, both pre-processing and data augmentation, and compare performance of all models based on 1. accuracy and 2. self driving in the unity simulator.

#Recapping the data acquisition process.

## Generating data
1. Start Unity Hub
% ./UnityHub.AppImage
2. In Unity "Scenes" directory, select "menu" scene and click run.
3. Choose option "Auto Drive w Rec"
4. Stop after a number of laps are completed.
5. Data is saved to ../sdsim/log
At this point there should be a number of .jpeg files in the path:
$ ls -l ../sdsim/log/*.jpg | wc -l
34440
In this case, about 12 laps generated 34440 images, plus the corresponding number of .json files with corresponding steering angle values. For comparision, the number of files generated by recording PID self-driving on a number of random tracks, as placed in datasets/unity/log2 path is approximately 561487/2 (to account for .jpeg and .json files)
# find . -maxdepth 1 -type d -print0 | while read -d '' -r dir; do num=$(find $dir -ls | wc -l); printf "%5d files in directory %s\n" "$num" "$dir"; done

That means to generate 561487/2 images with the "jungle" track, we would need around 80 laps ((561487/2) / 34440) * 10)

## Preparing data for training
The data is moved to ../dataset
$ python prepare_data.py --out-path=../dataset/unity/jungle1

Now we can run our 3 experiments, with this much reduced, although much more regular in terms of track pattern, dataset.

To break it down further, next task is to create additional command line parameters, to better document the process, instead of editing source files:
--net=nvidia1 --aug=0|1 --preproc=0|1 --sa_format=json|csv
Where sa_format is the steering angle format, to be extracted from .json file or .csv file.

<!--
Topic - "Obtaining yaw from Ford IMU data"
Reply from Vora, Ankit (A.) <avora3@ford.com>, one of the ford dataset paper co-authors:

Hi Daniel,

The information you are looking for will be in the /pose_ground_truth message. That message represents the position and orientation of the vehicle. You will have to convert quaternions to yaw, pitch, roll angles and then use the yaw values.

Ankit

On the topic of converting the quaternion to yaw:
https://stackoverflow.com/questions/5782658/extracting-yaw-from-a-quaternion
-->

Preliminary results. Better accuracies for jungle model with augmentation and preprocessing. We should try the adding to test set preprocessing all except random shifting. Meaning, add those steps to preprocessing, while augmentation would be only flipping, shifting and reflections.

Although accuracies are not SOTA, the unity sim self driving car has successfully completed several laps of the small_looping_course

2020.11.08

For the sake of quicker compilation, individual sections are to be written in a separate MSc_single_chapter overleaf project, then incorporated to main.tex in project report.

The main reflections from the self-driving models created yesterday are:
* What is the least amount of data required to create a model that could drive a lap around a circuit and a random track? If image preprocessing and augmentation are key, it could turn out to be less than expected.

Todos:
Copy and edit /home/simbox/git/msc-data/ford/sample/parse_ford_yaml_time.py to deal with steering angles. A few pointers:
https://www.theconstructsim.com/ros-qa-how-to-convert-quaternions-to-euler-angles/
https://robotics.stackexchange.com/questions/16471/get-yaw-from-quaternion
http://docs.ros.org/en/jade/api/tf/html/python/transformations.html
https://en.wikipedia.org/wiki/Euler_angles

To run NaokiNet we'll need to change the output format in 
./Assets/Prefabs/Donkey.prefab

  m_Name:
  m_EditorClassIdentifier:
  sensorCam: {fileID: 20774512129789012}
  width: 160
  height: 120

to 200w x 66h.

Either that or change the augmentation process, to first resize image then perform augmentation.

One thing to try first. Run udacity simulator in training mode and check image size output. That will be the ground truth. Expected size is greater than 200x66, both width and height.

2020.11.10

Ran Udacity sim, output images are 320w x 160h.































